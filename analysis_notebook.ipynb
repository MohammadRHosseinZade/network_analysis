{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Network Analysis - Bitcoin Alpha Signed Trust Network\n",
    "\n",
    "Complete analysis with metrics, visualizations, and report generation\n",
    "\n",
    "## Dataset Information\n",
    "- **Source:** Stanford SNAP network datasets\n",
    "- **Name:** Bitcoin Alpha signed trust network\n",
    "- **File:** soc-sign-bitcoinalpha.csv\n",
    "- **Type:** Directed weighted signed network\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Import libraries\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file (no header)\n",
    "df = pd.read_csv('soc-sign-bitcoinalpha.csv', header=None)\n",
    "\n",
    "# Assign column names manually\n",
    "df.columns = [\"source\", \"target\", \"rating\", \"time\"]\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "\n",
    "# Verify node ids are integers\n",
    "print(f\"\\nSource column type: {df['source'].dtype}\")\n",
    "print(f\"Target column type: {df['target'].dtype}\")\n",
    "\n",
    "# Build directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with rating as weight\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(row['source'], row['target'], weight=row['rating'])\n",
    "\n",
    "print(f\"\\nGraph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Basic Graph Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basic_metrics(G):\n",
    "    \"\"\"Compute basic graph metrics\"\"\"\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    \n",
    "    # Directed density\n",
    "    max_edges = n_nodes * (n_nodes - 1)\n",
    "    density = n_edges / max_edges if max_edges > 0 else 0\n",
    "    \n",
    "    # Average degree (for directed: total edges / nodes)\n",
    "    avg_degree = (2 * n_edges) / n_nodes if n_nodes > 0 else 0\n",
    "    \n",
    "    # Max degree\n",
    "    degrees = dict(G.degree())\n",
    "    max_degree = max(degrees.values()) if degrees else 0\n",
    "    \n",
    "    # Degree variance\n",
    "    degree_values = list(degrees.values())\n",
    "    degree_variance = np.var(degree_values) if degree_values else 0\n",
    "    \n",
    "    return {\n",
    "        'number_of_nodes': n_nodes,\n",
    "        'number_of_edges': n_edges,\n",
    "        'directed_density': density,\n",
    "        'average_degree': avg_degree,\n",
    "        'max_degree': max_degree,\n",
    "        'degree_variance': degree_variance\n",
    "    }\n",
    "\n",
    "basic_metrics = compute_basic_metrics(G)\n",
    "\n",
    "print(\"Basic Graph Metrics:\")\n",
    "for key, value in basic_metrics.items():\n",
    "    print(f\"  {key}: {value:.6f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "# Store in dataframe\n",
    "basic_metrics_df = pd.DataFrame([basic_metrics])\n",
    "print(\"\\nBasic Metrics DataFrame:\")\n",
    "print(basic_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weakly connected components\n",
    "wcc = list(nx.weakly_connected_components(G))\n",
    "wcc_sizes = [len(comp) for comp in wcc]\n",
    "largest_wcc_size = max(wcc_sizes) if wcc_sizes else 0\n",
    "max_wcc = max(wcc, key=len) if wcc else set()\n",
    "\n",
    "print(f\"Number of weakly connected components: {len(wcc)}\")\n",
    "print(f\"Largest WCC size: {largest_wcc_size}\")\n",
    "\n",
    "# Strongly connected components\n",
    "scc = list(nx.strongly_connected_components(G))\n",
    "scc_sizes = [len(comp) for comp in scc]\n",
    "largest_scc_size = max(scc_sizes) if scc_sizes else 0\n",
    "\n",
    "print(f\"\\nNumber of strongly connected components: {len(scc)}\")\n",
    "print(f\"Largest SCC size: {largest_scc_size}\")\n",
    "\n",
    "# Create subgraph of largest WCC\n",
    "G_lcc = G.subgraph(max_wcc).copy()\n",
    "print(f\"\\nLargest WCC subgraph: {G_lcc.number_of_nodes()} nodes, {G_lcc.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Distance Metrics (on largest WCC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_metrics(G_lcc):\n",
    "    \"\"\"Compute distance metrics on largest WCC\"\"\"\n",
    "    print(\"Computing distance metrics (this may take a while)...\")\n",
    "    \n",
    "    # Check if graph is connected\n",
    "    if not nx.is_weakly_connected(G_lcc):\n",
    "        print(\"Warning: Graph is not weakly connected, using largest component\")\n",
    "        wcc_lcc = list(nx.weakly_connected_components(G_lcc))\n",
    "        G_lcc = G_lcc.subgraph(max(wcc_lcc, key=len)).copy()\n",
    "    \n",
    "    # Convert to undirected for distance calculations\n",
    "    G_undirected = G_lcc.to_undirected()\n",
    "    \n",
    "    # For large graphs, use approximate algorithms\n",
    "    n_nodes = G_undirected.number_of_nodes()\n",
    "    \n",
    "    if n_nodes > 1000:\n",
    "        print(f\"Large graph ({n_nodes} nodes), using approximate algorithms...\")\n",
    "        \n",
    "        # Approximate diameter using sampling\n",
    "        try:\n",
    "            diameter = nx.approximation.diameter(G_undirected)\n",
    "        except:\n",
    "            # Fallback: sample shortest paths\n",
    "            sample_nodes = list(G_undirected.nodes())[:min(100, n_nodes)]\n",
    "            max_path = 0\n",
    "            for node in sample_nodes:\n",
    "                paths = nx.single_source_shortest_path_length(G_undirected, node)\n",
    "                if paths:\n",
    "                    max_path = max(max_path, max(paths.values()))\n",
    "            diameter = max_path\n",
    "        \n",
    "        # Approximate radius\n",
    "        try:\n",
    "            radius = nx.approximation.radius(G_undirected)\n",
    "        except:\n",
    "            # Fallback: sample eccentricities\n",
    "            sample_nodes = list(G_undirected.nodes())[:min(100, n_nodes)]\n",
    "            min_ecc = float('inf')\n",
    "            for node in sample_nodes:\n",
    "                ecc = nx.eccentricity(G_undirected, node)\n",
    "                min_ecc = min(min_ecc, ecc)\n",
    "            radius = min_ecc if min_ecc != float('inf') else 0\n",
    "        \n",
    "        # Approximate average shortest path length\n",
    "        print(\"Computing average shortest path length (sampling)...\")\n",
    "        sample_size = min(100, n_nodes)\n",
    "        sample_nodes = np.random.choice(list(G_undirected.nodes()), \n",
    "                                        size=min(sample_size, n_nodes), \n",
    "                                        replace=False)\n",
    "        path_lengths = []\n",
    "        for node in sample_nodes:\n",
    "            paths = nx.single_source_shortest_path_length(G_undirected, node)\n",
    "            path_lengths.extend([v for v in paths.values() if v > 0])\n",
    "        \n",
    "        avg_shortest_path = np.mean(path_lengths) if path_lengths else 0\n",
    "    else:\n",
    "        # Exact computation for smaller graphs\n",
    "        print(\"Computing exact distance metrics...\")\n",
    "        diameter = nx.diameter(G_undirected)\n",
    "        radius = nx.radius(G_undirected)\n",
    "        avg_shortest_path = nx.average_shortest_path_length(G_undirected)\n",
    "    \n",
    "    return {\n",
    "        'diameter': diameter,\n",
    "        'radius': radius,\n",
    "        'average_shortest_path_length': avg_shortest_path\n",
    "    }\n",
    "\n",
    "distance_metrics = compute_distance_metrics(G_lcc)\n",
    "\n",
    "print(\"\\nDistance Metrics:\")\n",
    "for key, value in distance_metrics.items():\n",
    "    print(f\"  {key}: {value:.6f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to undirected for clustering\n",
    "G_u = G.to_undirected()\n",
    "\n",
    "# Global clustering coefficient (transitivity)\n",
    "clustering_coeff = nx.transitivity(G_u)\n",
    "\n",
    "print(f\"Global Clustering Coefficient (Transitivity): {clustering_coeff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 - Freeman Degree Centralization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freeman_centralization(G):\n",
    "    \"\"\"Compute Freeman degree centralization\"\"\"\n",
    "    degrees = dict(G.degree())\n",
    "    if not degrees:\n",
    "        return 0\n",
    "    \n",
    "    max_degree = max(degrees.values())\n",
    "    degree_values = list(degrees.values())\n",
    "    n = len(degree_values)\n",
    "    \n",
    "    # Theoretical maximum: (n-1) * (n-2) for directed graph\n",
    "    theoretical_max = (n - 1) * (n - 2) if n > 2 else 1\n",
    "    \n",
    "    # Sum of (max_degree - degree_i)\n",
    "    sum_diff = sum(max_degree - d for d in degree_values)\n",
    "    \n",
    "    # Freeman centralization\n",
    "    freeman_centralization = sum_diff / theoretical_max if theoretical_max > 0 else 0\n",
    "    \n",
    "    return freeman_centralization\n",
    "\n",
    "freeman_centralization = compute_freeman_centralization(G)\n",
    "\n",
    "print(f\"Freeman Degree Centralization: {freeman_centralization:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 - Centrality Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing centrality metrics for all nodes (this may take a while)...\")\n",
    "\n",
    "# Degree centrality\n",
    "print(\"  Computing degree centrality...\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Closeness centrality (on largest WCC for connected graph)\n",
    "print(\"  Computing closeness centrality...\")\n",
    "try:\n",
    "    # Compute for all nodes in largest WCC at once\n",
    "    closeness_centrality_lcc = nx.closeness_centrality(G_lcc)\n",
    "    # Extend to all nodes\n",
    "    closeness_centrality = {n: closeness_centrality_lcc.get(n, 0) for n in G.nodes()}\n",
    "except:\n",
    "    print(\"    Closeness centrality computation failed, using approximate method...\")\n",
    "    closeness_centrality = {n: 0 for n in G.nodes()}\n",
    "    # Compute only for sample of nodes\n",
    "    sample_nodes = list(G_lcc.nodes())[:min(500, G_lcc.number_of_nodes())]\n",
    "    for node in sample_nodes:\n",
    "        try:\n",
    "            closeness_centrality[node] = nx.closeness_centrality(G_lcc, node)\n",
    "        except:\n",
    "            closeness_centrality[node] = 0\n",
    "\n",
    "# Betweenness centrality (approximate for large graphs)\n",
    "print(\"  Computing betweenness centrality...\")\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"    Using approximate betweenness (sampling)...\")\n",
    "    sample_size = min(100, G.number_of_nodes())\n",
    "    sample_nodes = np.random.choice(list(G.nodes()), \n",
    "                                    size=min(sample_size, G.number_of_nodes()), \n",
    "                                    replace=False)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, k=sample_size)\n",
    "else:\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Eigenvector centrality\n",
    "print(\"  Computing eigenvector centrality...\")\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "except:\n",
    "    print(\"    Eigenvector centrality failed, using PageRank as fallback\")\n",
    "    eigenvector_centrality = nx.pagerank(G)\n",
    "\n",
    "# PageRank - compute on largest WCC for efficiency\n",
    "print(\"  Computing PageRank...\")\n",
    "try:\n",
    "    # Compute on largest WCC (more efficient and usually sufficient)\n",
    "    # Use a more lenient tolerance for convergence\n",
    "    pagerank_lcc = nx.pagerank(G_lcc, max_iter=500, tol=1e-02, damping=0.85)\n",
    "    # Extend to all nodes\n",
    "    pagerank = {n: pagerank_lcc.get(n, 0) for n in G.nodes()}\n",
    "    print(\"    PageRank computed successfully on largest WCC\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        print(f\"    PageRank on WCC failed ({str(e)}), trying with even more relaxed parameters...\")\n",
    "        pagerank_lcc = nx.pagerank(G_lcc, max_iter=1000, tol=1e-01, damping=0.85)\n",
    "        pagerank = {n: pagerank_lcc.get(n, 0) for n in G.nodes()}\n",
    "        print(\"    PageRank computed with relaxed parameters\")\n",
    "    except:\n",
    "        try:\n",
    "            print(\"    Trying full graph with very relaxed parameters...\")\n",
    "            pagerank = nx.pagerank(G, max_iter=500, tol=1e-01, damping=0.85)\n",
    "            print(\"    PageRank computed on full graph\")\n",
    "        except:\n",
    "            print(\"    PageRank failed completely, using normalized degree centrality as fallback...\")\n",
    "            # Use normalized degree as fallback\n",
    "            max_deg = max(degree_centrality.values()) if degree_centrality.values() else 1\n",
    "            pagerank = {n: degree_centrality.get(n, 0) / max_deg if max_deg > 0 else 0 \n",
    "                       for n in G.nodes()}\n",
    "\n",
    "# Create centrality dataframe\n",
    "centrality_df = pd.DataFrame({\n",
    "    'node': list(G.nodes()),\n",
    "    'degree': [degree_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'closeness': [closeness_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'betweenness': [betweenness_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'eigenvector': [eigenvector_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'pagerank': [pagerank.get(n, 0) for n in G.nodes()]\n",
    "})\n",
    "\n",
    "print(f\"\\nCentrality DataFrame shape: {centrality_df.shape}\")\n",
    "print(\"\\nFirst 5 rows of centrality DataFrame:\")\n",
    "print(centrality_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 - Top Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_nodes(centrality_df, metric, n=3):\n",
    "    \"\"\"Get top N nodes for a given centrality metric\"\"\"\n",
    "    top = centrality_df.nlargest(n, metric)[['node', metric]]\n",
    "    return top\n",
    "\n",
    "top_nodes = {}\n",
    "for metric in ['degree', 'closeness', 'betweenness', 'eigenvector', 'pagerank']:\n",
    "    top = get_top_nodes(centrality_df, metric, n=3)\n",
    "    top_nodes[metric] = top\n",
    "    print(f\"\\nTop 3 nodes by {metric} centrality:\")\n",
    "    print(top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10 - Graph Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating graph visualization (this may take a while)...\")\n",
    "\n",
    "# Use largest WCC for visualization\n",
    "G_viz = G_lcc.copy()\n",
    "\n",
    "# Get node attributes\n",
    "nodes = list(G_viz.nodes())\n",
    "pagerank_values = [pagerank.get(n, 0) for n in nodes]\n",
    "degree_cent_values = [degree_centrality.get(n, 0) for n in nodes]\n",
    "\n",
    "# Normalize for visualization\n",
    "pagerank_norm = np.array(pagerank_values)\n",
    "pagerank_norm = (pagerank_norm - pagerank_norm.min()) / (pagerank_norm.max() - pagerank_norm.min() + 1e-10)\n",
    "node_sizes = 100 + 5000 * pagerank_norm\n",
    "\n",
    "degree_cent_norm = np.array(degree_cent_values)\n",
    "degree_cent_norm = (degree_cent_norm - degree_cent_norm.min()) / (degree_cent_norm.max() - degree_cent_norm.min() + 1e-10)\n",
    "\n",
    "# Get top 20 nodes by PageRank for labels\n",
    "top_20_nodes = centrality_df.nlargest(20, 'pagerank')['node'].tolist()\n",
    "node_labels = {n: str(n) if n in top_20_nodes else '' for n in nodes}\n",
    "\n",
    "# Layout\n",
    "print(\"  Computing layout...\")\n",
    "pos = nx.spring_layout(G_viz, seed=42, k=0.1, iterations=50)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw_networkx_nodes(G_viz, pos, \n",
    "                       node_size=node_sizes,\n",
    "                       node_color=degree_cent_norm,\n",
    "                       cmap=plt.cm.viridis,\n",
    "                       alpha=0.7)\n",
    "nx.draw_networkx_edges(G_viz, pos, \n",
    "                       alpha=0.1,\n",
    "                       width=0.5,\n",
    "                       arrows=True,\n",
    "                       arrowsize=10)\n",
    "nx.draw_networkx_labels(G_viz, pos, \n",
    "                        labels=node_labels,\n",
    "                        font_size=8,\n",
    "                        font_weight='bold')\n",
    "\n",
    "plt.title('Bitcoin Alpha Network Visualization\\n(Node size: PageRank, Color: Degree Centrality)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graph_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: graph_visualization.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11 - Distribution Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "degree_counts = pd.Series(degrees).value_counts().sort_index()\n",
    "\n",
    "# Plot 1: Degree distribution histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(degrees, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Degree', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Degree Distribution Histogram', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('degree_distribution_histogram.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: degree_distribution_histogram.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Log-log degree distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "nonzero_degrees = [d for d in degree_counts.index if d > 0]\n",
    "nonzero_counts = [degree_counts[d] for d in nonzero_degrees]\n",
    "plt.loglog(nonzero_degrees, nonzero_counts, 'o', markersize=6, alpha=0.7)\n",
    "plt.xlabel('Degree (log scale)', fontsize=12)\n",
    "plt.ylabel('Frequency (log scale)', fontsize=12)\n",
    "plt.title('Log-Log Degree Distribution', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('degree_distribution_loglog.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: degree_distribution_loglog.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Centrality distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "centrality_metrics = ['degree', 'closeness', 'betweenness', 'eigenvector', 'pagerank']\n",
    "for idx, metric in enumerate(centrality_metrics):\n",
    "    ax = axes[idx]\n",
    "    values = centrality_df[metric].values\n",
    "    values = values[values > 0]  # Remove zeros for better visualization\n",
    "    ax.hist(values, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(metric.capitalize(), fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.set_title(f'{metric.capitalize()} Centrality Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('centrality_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: centrality_distributions.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Bar chart of top nodes per metric\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(centrality_metrics):\n",
    "    ax = axes[idx]\n",
    "    top = get_top_nodes(centrality_df, metric, n=10)\n",
    "    ax.barh(range(len(top)), top[metric].values, alpha=0.7)\n",
    "    ax.set_yticks(range(len(top)))\n",
    "    ax.set_yticklabels(top['node'].values, fontsize=8)\n",
    "    ax.set_xlabel(metric.capitalize(), fontsize=10)\n",
    "    ax.set_title(f'Top 10 Nodes by {metric.capitalize()}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_nodes_bar_charts.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Saved: top_nodes_bar_charts.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "correlation_metrics = ['degree', 'closeness', 'betweenness', 'eigenvector', 'pagerank']\n",
    "corr_data = centrality_df[correlation_metrics]\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr = corr_data.corr(method='pearson')\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(pearson_corr)\n",
    "\n",
    "# Spearman correlation\n",
    "spearman_corr = corr_data.corr(method='spearman')\n",
    "print(\"\\nSpearman Correlation Matrix:\")\n",
    "print(spearman_corr)\n",
    "\n",
    "# Create heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pearson heatmap\n",
    "sns.heatmap(pearson_corr, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Pearson Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Spearman heatmap\n",
    "sns.heatmap(spearman_corr, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "axes[1].set_title('Spearman Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('centrality_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n  Saved: centrality_correlation_heatmap.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13 - Results Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics\n",
    "final_metrics = {\n",
    "    'nodes': basic_metrics['number_of_nodes'],\n",
    "    'edges': basic_metrics['number_of_edges'],\n",
    "    'density': basic_metrics['directed_density'],\n",
    "    'diameter': distance_metrics['diameter'],\n",
    "    'radius': distance_metrics['radius'],\n",
    "    'avg_degree': basic_metrics['average_degree'],\n",
    "    'degree_variance': basic_metrics['degree_variance'],\n",
    "    'freeman_centralization': freeman_centralization,\n",
    "    'max_degree': basic_metrics['max_degree'],\n",
    "    'largest_WCC': largest_wcc_size,\n",
    "    'largest_SCC': largest_scc_size,\n",
    "    'clustering_coefficient': clustering_coeff\n",
    "}\n",
    "\n",
    "metrics_table = pd.DataFrame([final_metrics])\n",
    "print(\"Final Metrics Table:\")\n",
    "print(metrics_table)\n",
    "\n",
    "# Export to CSV\n",
    "metrics_table.to_csv('metrics_table.csv', index=False)\n",
    "print(\"\\n  Saved: metrics_table.csv\")\n",
    "\n",
    "# Export centrality table\n",
    "centrality_df.to_csv('centrality_table.csv', index=False)\n",
    "print(\"  Saved: centrality_table.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14 - Report Generation\n",
    "\n",
    "The markdown report has been generated and saved as `report.md`. \n",
    "Check the generated files:\n",
    "- `metrics_table.csv` - Complete structural metrics\n",
    "- `centrality_table.csv` - Centrality values for all nodes\n",
    "- `graph_visualization.png` - Network visualization\n",
    "- `degree_distribution_histogram.png` - Degree distribution\n",
    "- `degree_distribution_loglog.png` - Log-log degree plot\n",
    "- `centrality_distributions.png` - Centrality distributions\n",
    "- `top_nodes_bar_charts.png` - Top nodes visualization\n",
    "- `centrality_correlation_heatmap.png` - Correlation heatmaps\n",
    "- `report.md` - Comprehensive markdown report\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
